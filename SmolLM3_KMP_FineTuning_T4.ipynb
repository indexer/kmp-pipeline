{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning SmolLM3-3B for Kotlin Multiplatform Code Generation\n",
    "\n",
    "**Optimized for Google Colab T4 GPU**\n",
    "\n",
    "This notebook fine-tunes SmolLM3-3B on Kotlin KMP code with **full method body implementations**.\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ **QLoRA** for efficient training on T4 (15GB VRAM)\n",
    "- ‚úÖ **Method body validation** - Tests if model generates full implementations\n",
    "- ‚úÖ **Production-ready** - Gradient checkpointing, mixed precision\n",
    "- ‚úÖ **Quality metrics** - Evaluates implementation vs signature-only generation\n",
    "\n",
    "## Dataset Stats:\n",
    "- **132,577** training pairs\n",
    "- **52.5%** have real method bodies\n",
    "- **7 pair types**: expect/actual, interface‚Üíimpl, description‚Üícode, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies\n",
    "!pip install -q transformers==4.48.0 datasets==3.2.0 peft==0.14.0 bitsandbytes==0.45.0 trl==0.12.0 accelerate==1.2.1 sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import re\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURE YOUR DATA PATH ===\n",
    "# Upload your data/final_training/ folder to Google Drive first\n",
    "DATA_DIR = \"/content/drive/MyDrive/kmp_training_data/final_training\"\n",
    "\n",
    "# Verify files exist\n",
    "!ls -lh {DATA_DIR}/*.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "def load_kmp_dataset(data_dir):\n",
    "    \"\"\"Load KMP training data from JSONL files\"\"\"\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    # Load training pairs\n",
    "    train_file = f\"{data_dir}/train.jsonl\"\n",
    "    val_file = f\"{data_dir}/val.jsonl\"\n",
    "    \n",
    "    print(f\"Loading training data from {train_file}...\")\n",
    "    with open(train_file, 'r') as f:\n",
    "        for line in f:\n",
    "            train_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Loading validation data from {val_file}...\")\n",
    "    with open(val_file, 'r') as f:\n",
    "        for line in f:\n",
    "            val_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"\\nDataset loaded:\")\n",
    "    print(f\"  Train: {len(train_data):,} examples\")\n",
    "    print(f\"  Val:   {len(val_data):,} examples\")\n",
    "    \n",
    "    return Dataset.from_list(train_data), Dataset.from_list(val_data)\n",
    "\n",
    "train_dataset, val_dataset = load_kmp_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Get pair type distribution\n",
    "pair_types = Counter([item['pair_type'] for item in train_dataset])\n",
    "\n",
    "print(\"\\nüìä Pair Type Distribution:\")\n",
    "for ptype, count in pair_types.most_common():\n",
    "    pct = 100 * count / len(train_dataset)\n",
    "    print(f\"  {ptype:35s} {count:>7,}  ({pct:5.1f}%)\")\n",
    "\n",
    "# Sample examples\n",
    "print(\"\\nüìù Sample Training Example:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nPair Type: {sample.get('pair_type', 'unknown')}\")\n",
    "print(f\"Source Set: {sample.get('source_set', 'unknown')}\")\n",
    "print(f\"\\nINPUT (first 200 chars):\\n{sample['input_text'][:200]}...\")\n",
    "print(f\"\\nTARGET (first 300 chars):\\n{sample['target_text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model with QLoRA (T4 Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"  # Using 1.7B for T4 safety\n",
    "# Alternative: \"HuggingFaceTB/SmolLM3-3B\" if you have more VRAM\n",
    "\n",
    "# QLoRA configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(f\"Quantization: 4-bit NF4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded:\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                       # LoRA rank\n",
    "    lora_alpha=32,              # LoRA alpha (scaling factor)\n",
    "    target_modules=[            # Target attention modules\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"\n",
    "    Format training example into instruction format.\n",
    "    \n",
    "    Uses a clear instruction format that emphasizes FULL implementation.\n",
    "    \"\"\"\n",
    "    input_text = example['input_text']\n",
    "    target_text = example['target_text']\n",
    "    \n",
    "    # Create instruction-following format\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are an expert Kotlin Multiplatform developer. Generate complete, working code with full method body implementations. Never output just signatures or TODO comments.<|im_end|>\n",
    "<|im_start|>user\n",
    "{input_text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{target_text}<|im_end|>\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Format datasets\n",
    "train_dataset_formatted = train_dataset.map(format_prompt)\n",
    "val_dataset_formatted = val_dataset.map(format_prompt)\n",
    "\n",
    "# Show example\n",
    "print(\"\\nüìù Formatted Example (first 500 chars):\")\n",
    "print(train_dataset_formatted[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for T4\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./smollm3-kmp-finetuned\",\n",
    "    \n",
    "    # Training regime\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,      # Small batch for T4\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,       # Effective batch size = 16\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,                           # BFloat16 mixed precision\n",
    "    tf32=True,\n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Other\n",
    "    report_to=\"none\",                    # Disable wandb/tensorboard\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision: {'BF16' if training_args.bf16 else 'FP16' if training_args.fp16 else 'FP32'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=val_dataset_formatted,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    packing=False,  # Don't pack sequences (clearer for evaluation)\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Trainer initialized!\")\n",
    "print(f\"   Training samples: {len(train_dataset_formatted):,}\")\n",
    "print(f\"   Eval samples: {len(val_dataset_formatted):,}\")\n",
    "print(f\"   Max sequence length: 2048 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "output_dir = \"./smollm3-kmp-final\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {output_dir}\")\n",
    "\n",
    "# Optionally save to Google Drive\n",
    "import shutil\n",
    "drive_save_path = \"/content/drive/MyDrive/smollm3-kmp-finetuned\"\n",
    "shutil.copytree(output_dir, drive_save_path, dirs_exist_ok=True)\n",
    "print(f\"‚úÖ Model backed up to Google Drive: {drive_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test: Method Body Implementation Quality\n",
    "\n",
    "**Critical Test**: Does the model generate FULL implementations or just signatures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(prompt, model, tokenizer, max_new_tokens=512):\n",
    "    \"\"\"Generate code completion\"\"\"\n",
    "    \n",
    "    # Format prompt\n",
    "    formatted_prompt = f\"\"\"<|im_start|>system\n",
    "You are an expert Kotlin Multiplatform developer. Generate complete, working code with full method body implementations. Never output just signatures or TODO comments.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\\n\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    if \"<|im_start|>assistant\" in generated:\n",
    "        response = generated.split(\"<|im_start|>assistant\")[-1]\n",
    "        response = response.replace(\"<|im_end|>\", \"\").strip()\n",
    "        return response\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"‚úÖ Generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_method_body_quality(code):\n",
    "    \"\"\"\n",
    "    Analyze generated code to check if it has real method bodies.\n",
    "    \n",
    "    Returns:\n",
    "        dict with quality metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"has_braces\": \"{\" in code and \"}\" in code,\n",
    "        \"has_return\": \"return \" in code,\n",
    "        \"has_logic\": False,\n",
    "        \"has_todo\": \"TODO\" in code or \"todo\" in code.lower(),\n",
    "        \"has_empty_body\": \"{ }\" in code or \"{\\n}\" in code,\n",
    "        \"quality_score\": 0,\n",
    "    }\n",
    "    \n",
    "    # Check for real implementation logic\n",
    "    logic_indicators = [\n",
    "        \"if (\", \"when (\", \"for (\", \"while (\",\n",
    "        \".map\", \".filter\", \".collect\", \".launch\",\n",
    "        \"try {\", \"catch\", \"emit(\",\n",
    "        \"= \", \"+=\", \"-=\",\n",
    "    ]\n",
    "    \n",
    "    logic_count = sum(1 for indicator in logic_indicators if indicator in code)\n",
    "    results[\"has_logic\"] = logic_count >= 2\n",
    "    \n",
    "    # Calculate quality score\n",
    "    score = 0\n",
    "    if results[\"has_braces\"]: score += 1\n",
    "    if results[\"has_return\"]: score += 1\n",
    "    if results[\"has_logic\"]: score += 2\n",
    "    if not results[\"has_todo\"]: score += 1\n",
    "    if not results[\"has_empty_body\"]: score += 1\n",
    "    \n",
    "    results[\"quality_score\"] = score\n",
    "    results[\"is_full_implementation\"] = score >= 4\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Quality checker ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 1: Interface Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_1 = \"\"\"// Source set: commonMain\n",
    "// Implement this interface with full method bodies:\n",
    "interface UserRepository {\n",
    "    suspend fun getUser(id: String): User\n",
    "    suspend fun saveUser(user: User)\n",
    "}\"\"\"\n",
    "\n",
    "print(\"üß™ Test 1: Interface ‚Üí Implementation\\n\")\n",
    "print(\"INPUT:\")\n",
    "print(test_prompt_1)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "generated = generate_code(test_prompt_1, trainer.model, tokenizer, max_new_tokens=512)\n",
    "print(\"GENERATED:\")\n",
    "print(generated)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "quality = check_method_body_quality(generated)\n",
    "print(\"QUALITY ANALYSIS:\")\n",
    "print(f\"  Has braces: {quality['has_braces']}\")\n",
    "print(f\"  Has return statements: {quality['has_return']}\")\n",
    "print(f\"  Has logic (if/when/etc): {quality['has_logic']}\")\n",
    "print(f\"  Has TODO: {quality['has_todo']}\")\n",
    "print(f\"  Has empty body: {quality['has_empty_body']}\")\n",
    "print(f\"  Quality score: {quality['quality_score']}/6\")\n",
    "print(f\"  ‚úÖ Full implementation: {quality['is_full_implementation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 2: Expect/Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_2 = \"\"\"// Source set: androidMain\n",
    "// Implement the actual for this expect declaration:\n",
    "expect class PlatformLogger() {\n",
    "    fun log(message: String)\n",
    "}\"\"\"\n",
    "\n",
    "print(\"üß™ Test 2: Expect ‚Üí Actual\\n\")\n",
    "print(\"INPUT:\")\n",
    "print(test_prompt_2)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "generated = generate_code(test_prompt_2, trainer.model, tokenizer, max_new_tokens=512)\n",
    "print(\"GENERATED:\")\n",
    "print(generated)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "quality = check_method_body_quality(generated)\n",
    "print(\"QUALITY ANALYSIS:\")\n",
    "print(f\"  Quality score: {quality['quality_score']}/6\")\n",
    "print(f\"  ‚úÖ Full implementation: {quality['is_full_implementation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 3: ViewModel with State Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_3 = \"\"\"// Source set: commonMain\n",
    "// Implement ViewModel 'LoginViewModel' with state management, coroutines, and event handling\n",
    "\n",
    "class LoginViewModel\"\"\"\n",
    "\n",
    "print(\"üß™ Test 3: ViewModel Implementation\\n\")\n",
    "print(\"INPUT:\")\n",
    "print(test_prompt_3)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "generated = generate_code(test_prompt_3, trainer.model, tokenizer, max_new_tokens=800)\n",
    "print(\"GENERATED:\")\n",
    "print(generated)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "quality = check_method_body_quality(generated)\n",
    "print(\"QUALITY ANALYSIS:\")\n",
    "print(f\"  Quality score: {quality['quality_score']}/6\")\n",
    "print(f\"  ‚úÖ Full implementation: {quality['is_full_implementation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 4: Composable UI Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt_4 = \"\"\"// Source set: commonMain\n",
    "// Implement @Composable function 'LoginScreen' with full UI layout:\n",
    "\n",
    "@Composable\n",
    "fun LoginScreen()\"\"\"\n",
    "\n",
    "print(\"üß™ Test 4: Composable UI\\n\")\n",
    "print(\"INPUT:\")\n",
    "print(test_prompt_4)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "generated = generate_code(test_prompt_4, trainer.model, tokenizer, max_new_tokens=800)\n",
    "print(\"GENERATED:\")\n",
    "print(generated)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "quality = check_method_body_quality(generated)\n",
    "print(\"QUALITY ANALYSIS:\")\n",
    "print(f\"  Quality score: {quality['quality_score']}/6\")\n",
    "print(f\"  ‚úÖ Full implementation: {quality['is_full_implementation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_file = f\"{DATA_DIR}/test.jsonl\"\n",
    "test_data = []\n",
    "with open(test_file, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Test set: {len(test_data):,} examples\")\n",
    "\n",
    "# Sample for evaluation (use first 50 for speed)\n",
    "test_sample = test_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nüîÑ Evaluating on test set...\\n\")\n",
    "\n",
    "for i, example in enumerate(tqdm(test_sample, desc=\"Testing\")):\n",
    "    input_text = example['input_text']\n",
    "    expected = example['target_text']\n",
    "    \n",
    "    # Generate\n",
    "    generated = generate_code(input_text, trainer.model, tokenizer, max_new_tokens=512)\n",
    "    \n",
    "    # Check quality\n",
    "    quality = check_method_body_quality(generated)\n",
    "    expected_quality = check_method_body_quality(expected)\n",
    "    \n",
    "    results.append({\n",
    "        'pair_type': example.get('pair_type', 'unknown'),\n",
    "        'generated_quality': quality['quality_score'],\n",
    "        'expected_quality': expected_quality['quality_score'],\n",
    "        'is_full_impl': quality['is_full_implementation'],\n",
    "        'has_logic': quality['has_logic'],\n",
    "        'has_todo': quality['has_todo'],\n",
    "    })\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal samples evaluated: {len(df)}\")\n",
    "\n",
    "print(f\"\\nüéØ Implementation Quality:\")\n",
    "print(f\"  Full implementations: {df['is_full_impl'].sum()} / {len(df)} ({100*df['is_full_impl'].mean():.1f}%)\")\n",
    "print(f\"  Has logic (if/when/etc): {df['has_logic'].sum()} / {len(df)} ({100*df['has_logic'].mean():.1f}%)\")\n",
    "print(f\"  Has TODO markers: {df['has_todo'].sum()} / {len(df)} ({100*df['has_todo'].mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Quality Scores:\")\n",
    "print(f\"  Mean generated quality: {df['generated_quality'].mean():.2f} / 6\")\n",
    "print(f\"  Mean expected quality:  {df['expected_quality'].mean():.2f} / 6\")\n",
    "print(f\"  Quality retention:      {100*df['generated_quality'].mean()/df['expected_quality'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nüìã By Pair Type:\")\n",
    "for ptype in df['pair_type'].unique():\n",
    "    subset = df[df['pair_type'] == ptype]\n",
    "    full_impl_pct = 100 * subset['is_full_impl'].mean()\n",
    "    avg_quality = subset['generated_quality'].mean()\n",
    "    print(f\"  {ptype:30s} Full: {full_impl_pct:5.1f}%  Quality: {avg_quality:.2f}/6\")\n",
    "\n",
    "# Success criteria\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "full_impl_rate = 100 * df['is_full_impl'].mean()\n",
    "if full_impl_rate >= 70:\n",
    "    print(\"‚úÖ SUCCESS: Model generates full implementations (‚â•70%)\")\n",
    "elif full_impl_rate >= 50:\n",
    "    print(\"‚ö†Ô∏è  PARTIAL: Model sometimes generates full implementations (50-70%)\")\n",
    "else:\n",
    "    print(\"‚ùå FAILURE: Model mostly generates signatures/stubs (<50%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Training Complete\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saved\")\n",
    "print(f\"   Local: ./smollm3-kmp-final\")\n",
    "print(f\"   Drive: /content/drive/MyDrive/smollm3-kmp-finetuned\")\n",
    "\n",
    "if 'df' in globals():\n",
    "    print(f\"\\nüéØ Evaluation Results\")\n",
    "    print(f\"   Full implementations: {100*df['is_full_impl'].mean():.1f}%\")\n",
    "    print(f\"   Average quality: {df['generated_quality'].mean():.2f}/6\")\n",
    "\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"   1. Test on your own KMP code examples\")\n",
    "print(\"   2. Integrate with your IDE\")\n",
    "print(\"   3. Fine-tune further if needed\")\n",
    "print(\"   4. Deploy to production\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
